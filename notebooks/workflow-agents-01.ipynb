{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ed770b-f8b9-4647-ae81-35a4f5ed1517",
   "metadata": {},
   "source": [
    "# Workflows and Agents\n",
    "\n",
    "**Preamble**: There two primary patterns we can follow according to [Anthropic](https://www.anthropic.com/research/building-effective-agents) blog post.\n",
    "1. **Workflow**: \n",
    "    1. Create a scaffolding of predefined code paths around LLM calls\n",
    "    2. LLMs directs control flow through predefined code paths\n",
    "2. **Agent**: Remove this scaffolding (LLM directs its own **actions**, responds to **feedback**). In other words, agents don't have boundaries and they can make a decision (actions) based on the user input, context, etc.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "   <img src=\"images/worflow_agent.webp\" alt=\"Sample Image\" width=\"70%\">\n",
    "</div>\n",
    "\n",
    "**Why Frameworks?**\n",
    "\n",
    "- Implementing these patterns *does not* require a framework like LangGraph.\n",
    "- LangGraph aims to *minimize* overhead of implementing these patterns.\n",
    "- LangGraph provides supporting infrastructure underneath ``any workflow / agent``:\n",
    "    - **Persistence**\n",
    "        - Memory\n",
    "        - Human-In-The-Loop\n",
    "    - **Streaming** \n",
    "        - From any LLM call or step in workflow / agent\n",
    "    - **Deployment**\n",
    "        - Testing, debugging, and deploying\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca63d90-0592-4853-856e-76c279120844",
   "metadata": {},
   "source": [
    "## LLM Setup\n",
    "For the following notebooks, we use LangGraph along Anthropic API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3876a195-bee7-4b16-a581-f70682d73979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  System: darwin\n",
      "  Platform: macOS-15.4.1-arm64-arm-64bit\n",
      "  Python version: 3.11.12\n",
      "  System Execution (Python) path: .venv/bin/python\n",
      "  Last update: 2025-05-05\n"
     ]
    }
   ],
   "source": [
    "# Required Modules\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import platform\n",
    "from datetime import date, datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "print(f\"  System: {sys.platform}\")\n",
    "print(f\"  Platform: {platform.platform()}\")\n",
    "print(f\"  Python version: {platform.python_version()}\")\n",
    "print(f\"  System Execution (Python) path: {'/'.join(sys.executable.strip('/').split('/')[-3:])}\")\n",
    "print(f\"  Last update: {date.today().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eb862da-0a89-485e-915f-3ba0684efb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "805ebc65-b284-4837-989d-a706707228b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 12:33:32,734 - __main__ - INFO - Environment variable 'ANTHROPIC_API_KEY' is loaded.\n",
      "2025-05-05 12:33:32,736 - __main__ - INFO - LLM model object is built wih 'claude-3-5-sonnet-latest'.\n"
     ]
    }
   ],
   "source": [
    "def _set_env(var: str):\n",
    "    load_dotenv()  # Load variables from .env into os.environ\n",
    "    if not os.environ.get(var):\n",
    "        raise EnvironmentError(f\"Environment variable '{var}' not found. Please set it in .env\")\n",
    "    else:\n",
    "        logger.info(f\"Environment variable '{var}' is loaded.\")\n",
    "\n",
    "# Load \n",
    "_set_env(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "model = \"claude-3-5-sonnet-latest\"\n",
    "llm = ChatAnthropic(model=model)\n",
    "logger.info(f\"LLM model object is built wih '{model}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdeb386-4943-450c-8191-c92f70e134ce",
   "metadata": {},
   "source": [
    "# Augmented LLM\n",
    "General speaking LLM's, in our context, can be used for different puposes shown in th figure.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "   <img src=\"images/augmented_llm.webp\" alt=\"Sample Image\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "To make sense out of it, let's go through two examples:\n",
    "1. Search query\n",
    "2. Tool using"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b047f4b-3b4b-4477-9a54-52f71cb36799",
   "metadata": {},
   "source": [
    "## Search Query\n",
    "In this example, we want to answer the user question. This is similar to what we do using, for example, chatGPT.\n",
    "Note, that for building and agent we use [``Pydantic AI``](https://ai.pydantic.dev), which is a Python agent framework offering an innovative and ergonomic design.\n",
    "\n",
    "**Note**: This is not an agent. Rather, LLM is just being used in a predefined code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f6be7d1-13bc-48ee-9743-3771e6583341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 12:33:34,788 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-05 12:33:34,798 - __main__ - INFO - relationship between Calcium CT score and high cholesterol heart disease\n",
      "2025-05-05 12:33:34,799 - __main__ - INFO - Searching for information about the connection between coronary calcium scoring and cholesterol levels to understand their relationship in cardiovascular health assessment.\n"
     ]
    }
   ],
   "source": [
    "# Schema for structured output\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Query that is optimized web search.\")\n",
    "    justification: str = Field(\n",
    "        None, justification=\"Why this query is relevant to the user's request.\"\n",
    "    )\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "structured_llm = llm.with_structured_output(SearchQuery)\n",
    "\n",
    "# Invoke the augmented LLM\n",
    "output = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\n",
    "logger.info(output.search_query)\n",
    "logger.info(output.justification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138dd622-02c7-44c6-810b-71e3039c4cc9",
   "metadata": {},
   "source": [
    "## Tool Using\n",
    "In the second example, let's create a tool and use it to answer the user question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8807ddcd-434f-4209-8502-622ecb1d2780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 13:17:52,264 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 2, 'b': 3},\n",
       "  'id': 'toolu_01Nga8LhVy8t1FHE3M6K3Ag6',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    return a * b\n",
    "\n",
    "# Augment the LLM with tools\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "\n",
    "# Invoke the LLM with input that triggers the tool call\n",
    "msg = llm_with_tools.invoke(\"What is two times 3?\")\n",
    "\n",
    "# Get the tool call\n",
    "msg.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d868d5b-0203-453d-999d-1047e1dce65e",
   "metadata": {},
   "source": [
    "As you can see, the LLM model has access to the *multiply* tool with an ID. also, it identified the arguments (in this case 2 and 3) to be used via the tool. Now, this can be used later when we will build the execution components, or pass it to the next step in the chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d7d625-6ee6-4263-80ca-5435ccd554ad",
   "metadata": {},
   "source": [
    "ðŸ”… In the next notebooks, we will go through different patterns including:\n",
    "\n",
    "- Prompt Chaining\n",
    "- Prompt Paralleliztion\n",
    "- Routing\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530ec09-ab3d-4230-b69c-127e6004cf18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (agent-dev)",
   "language": "python",
   "name": "agent-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
